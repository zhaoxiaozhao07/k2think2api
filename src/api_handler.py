"""
APIå¤„ç†æ¨¡å—
å¤„ç†ä¸»è¦çš„APIè·¯ç”±é€»è¾‘
"""
import json
import time
import asyncio
import logging
from typing import Dict, List
from fastapi import HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse

from src.config import Config
from src.constants import (
    APIConstants, ResponseConstants, LogMessages, 
    ErrorMessages, HeaderConstants
)
from src.exceptions import (
    AuthenticationError, SerializationError, 
    K2ThinkProxyError, UpstreamError
)
from src.models import ChatCompletionRequest, ModelsResponse, ModelInfo
from src.response_processor import ResponseProcessor
from src.token_manager import TokenManager
from src.utils import safe_log_error, safe_log_info, safe_log_warning
from src.toolify_handler import should_enable_toolify, prepare_toolify_request

logger = logging.getLogger(__name__)

class APIHandler:
    """APIå¤„ç†å™¨"""
    
    def __init__(self, config: Config):
        self.config = config
        self.response_processor = ResponseProcessor(config)
        self.token_manager = config.get_token_manager()
    
    def validate_api_key(self, authorization: str) -> bool:
        """éªŒè¯APIå¯†é’¥"""
        if not authorization or not authorization.startswith(APIConstants.BEARER_PREFIX):
            return False
        api_key = authorization[APIConstants.BEARER_PREFIX_LENGTH:]  # ç§»é™¤ "Bearer " å‰ç¼€
        return api_key == self.config.VALID_API_KEY
    
    def should_output_thinking(self, model_name: str) -> bool:
        """æ ¹æ®æ¨¡å‹ååˆ¤æ–­æ˜¯å¦åº”è¯¥è¾“å‡ºæ€è€ƒå†…å®¹"""
        return model_name != APIConstants.MODEL_ID_NOTHINK
    
    def get_actual_model_id(self, model_name: str) -> str:
        """è·å–å®é™…çš„æ¨¡å‹IDï¼ˆå°†nothinkç‰ˆæœ¬æ˜ å°„å›åŸå§‹æ¨¡å‹ï¼‰"""
        if model_name == APIConstants.MODEL_ID_NOTHINK:
            return APIConstants.MODEL_ID
        return model_name
    
    async def get_models(self) -> ModelsResponse:
        """è·å–æ¨¡å‹åˆ—è¡¨"""
        model_info_standard = ModelInfo(
            id=APIConstants.MODEL_ID,
            created=int(time.time()),
            owned_by=APIConstants.MODEL_OWNER,
            root=APIConstants.MODEL_ROOT
        )
        model_info_nothink = ModelInfo(
            id=APIConstants.MODEL_ID_NOTHINK,
            created=int(time.time()),
            owned_by=APIConstants.MODEL_OWNER,
            root=APIConstants.MODEL_ROOT
        )
        return ModelsResponse(data=[model_info_standard, model_info_nothink])
    
    async def chat_completions(self, request: ChatCompletionRequest, auth_request: Request):
        """å¤„ç†èŠå¤©è¡¥å…¨è¯·æ±‚"""
        # éªŒè¯APIå¯†é’¥
        authorization = auth_request.headers.get(HeaderConstants.AUTHORIZATION, "")
        if not self.validate_api_key(authorization):
            raise AuthenticationError()
        
        # åˆ¤æ–­æ˜¯å¦åº”è¯¥è¾“å‡ºæ€è€ƒå†…å®¹
        output_thinking = self.should_output_thinking(request.model)
        actual_model_id = self.get_actual_model_id(request.model)
        
        try:
            # å¤„ç†æ¶ˆæ¯
            raw_messages = self._process_raw_messages(request.messages)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦å¯ç”¨å·¥å…·è°ƒç”¨
            request_dict = request.model_dump()
            enable_toolify = should_enable_toolify(request_dict)
            
            # å¦‚æœå¯ç”¨å·¥å…·è°ƒç”¨ï¼Œé¢„å¤„ç†æ¶ˆæ¯å¹¶æ³¨å…¥æç¤ºè¯
            if enable_toolify:
                safe_log_info(logger, "[TOOLIFY] å·¥å…·è°ƒç”¨åŠŸèƒ½å·²å¯ç”¨")
                raw_messages, _ = prepare_toolify_request(request_dict, raw_messages)
            
            self._log_request_info(raw_messages)
            
            # æ„å»ºK2Thinkè¯·æ±‚
            k2think_payload = self._build_k2think_payload(
                request, raw_messages, actual_model_id
            )
            
            # éªŒè¯JSONåºåˆ—åŒ–
            self._validate_json_serialization(k2think_payload)
            
            # å¤„ç†å“åº”ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
            if request.stream:
                return await self._handle_stream_response_with_retry(
                    request, k2think_payload, output_thinking, enable_toolify
                )
            else:
                return await self._handle_non_stream_response_with_retry(
                    request, k2think_payload, output_thinking, enable_toolify
                )
                
        except K2ThinkProxyError:
            # é‡æ–°æŠ›å‡ºè‡ªå®šä¹‰å¼‚å¸¸
            raise
        except Exception as e:
            safe_log_error(logger, "APIè½¬å‘é”™è¯¯", e)
            raise HTTPException(
                status_code=APIConstants.HTTP_INTERNAL_ERROR,
                detail={
                    "error": {
                        "message": str(e),
                        "type": ErrorMessages.API_ERROR
                    }
                }
            )
    
    def _process_raw_messages(self, messages: List) -> List[Dict]:
        """å¤„ç†åŸå§‹æ¶ˆæ¯"""
        raw_messages = []
        for msg in messages:
            try:
                raw_messages.append({
                    "role": msg.role, 
                    "content": msg.content  # ä¿æŒåŸå§‹æ ¼å¼ï¼Œç¨åå†è½¬æ¢
                })
            except Exception as e:
                safe_log_error(logger, f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™, æ¶ˆæ¯: {msg}", e)
                # ä½¿ç”¨é»˜è®¤å€¼
                raw_messages.append({
                    "role": msg.role, 
                    "content": str(msg.content) if msg.content else ""
                })
        return raw_messages
    
    def _log_request_info(self, raw_messages: List[Dict]):
        """è®°å½•è¯·æ±‚ä¿¡æ¯"""
        safe_log_info(logger, LogMessages.MESSAGE_RECEIVED.format(len(raw_messages)))
        
        # è®°å½•åŸå§‹æ¶ˆæ¯çš„è§’è‰²åˆ†å¸ƒ
        role_count = {}
        for msg in raw_messages:
            role = msg.get("role", "unknown")
            role_count[role] = role_count.get(role, 0) + 1
        safe_log_info(logger, LogMessages.ROLE_DISTRIBUTION.format("åŸå§‹", role_count))
    
    def _build_k2think_payload(
        self, 
        request: ChatCompletionRequest, 
        processed_messages: List[Dict],
        actual_model_id: str = None
    ) -> Dict:
        """æ„å»ºK2Thinkè¯·æ±‚è´Ÿè½½"""
        # æ„å»ºK2Thinkæ ¼å¼çš„è¯·æ±‚ä½“ - æ”¯æŒå¤šæ¨¡æ€å†…å®¹
        k2think_messages = []
        for msg in processed_messages:
            try:
                # ä½¿ç”¨å¤šæ¨¡æ€å†…å®¹è½¬æ¢å‡½æ•°
                content = self.response_processor.content_to_multimodal(msg.get("content", ""))
                k2think_messages.append({
                    "role": msg["role"], 
                    "content": content
                })
            except Exception as e:
                safe_log_error(logger, f"æ„å»ºK2Thinkæ¶ˆæ¯æ—¶å‡ºé”™, æ¶ˆæ¯: {msg}", e)
                # ä½¿ç”¨å®‰å…¨çš„é»˜è®¤å€¼
                fallback_content = str(msg.get("content", ""))
                k2think_messages.append({
                    "role": msg.get("role", "user"), 
                    "content": fallback_content
                })
        
        # ä½¿ç”¨å®é™…çš„æ¨¡å‹ID
        model_id = actual_model_id or APIConstants.MODEL_ID
        
        return {
            "stream": request.stream,
            "model": model_id,
            "messages": k2think_messages,
            "params": {},
            "tool_servers": [],
            "features": {
                "image_generation": False,
                "code_interpreter": False,
                "web_search": False
            },
            "variables": self.response_processor.get_current_datetime_info(),
            "model_item": {
                "id": model_id,
                "object": ResponseConstants.MODEL_OBJECT,
                "owned_by": APIConstants.MODEL_OWNER,
                "root": APIConstants.MODEL_ROOT,
                "parent": None,
                "status": "active",
                "connection_type": "external",
                "name": model_id
            },
            "background_tasks": {
                "title_generation": True,
                "tags_generation": True
            },
            "chat_id": self.response_processor.generate_chat_id(),
            "id": self.response_processor.generate_session_id(),
            "session_id": self.response_processor.generate_session_id()
        }
    
    def _validate_json_serialization(self, k2think_payload: Dict):
        """éªŒè¯JSONåºåˆ—åŒ–"""
        try:
            # æµ‹è¯•JSONåºåˆ—åŒ–
            json.dumps(k2think_payload, ensure_ascii=False)
            safe_log_info(logger, LogMessages.JSON_VALIDATION_SUCCESS)
        except Exception as e:
            safe_log_error(logger, LogMessages.JSON_VALIDATION_FAILED.format(e))
            # å°è¯•ä¿®å¤åºåˆ—åŒ–é—®é¢˜
            try:
                k2think_payload = json.loads(json.dumps(k2think_payload, default=str, ensure_ascii=False))
                safe_log_info(logger, LogMessages.JSON_FIXED)
            except Exception as fix_error:
                safe_log_error(logger, "æ— æ³•ä¿®å¤åºåˆ—åŒ–é—®é¢˜", fix_error)
                raise SerializationError()
    
    def _build_request_headers(self, request: ChatCompletionRequest, k2think_payload: Dict, token: str) -> Dict[str, str]:
        """æ„å»ºè¯·æ±‚å¤´"""
        return {
            HeaderConstants.ACCEPT: (
                HeaderConstants.EVENT_STREAM_JSON if request.stream 
                else HeaderConstants.APPLICATION_JSON
            ),
            HeaderConstants.CONTENT_TYPE: HeaderConstants.APPLICATION_JSON,
            HeaderConstants.AUTHORIZATION: f"{APIConstants.BEARER_PREFIX}{token}",
            HeaderConstants.ORIGIN: "https://www.k2think.ai",
            HeaderConstants.REFERER: "https://www.k2think.ai/c/" + k2think_payload["chat_id"],
            HeaderConstants.USER_AGENT: HeaderConstants.DEFAULT_USER_AGENT
        }
    
    async def _handle_stream_response(
        self, 
        k2think_payload: Dict, 
        headers: Dict[str, str],
        output_thinking: bool = True,
        original_model: str = None
    ) -> StreamingResponse:
        """å¤„ç†æµå¼å“åº”"""
        return StreamingResponse(
            self.response_processor.process_stream_response(
                k2think_payload, headers, output_thinking, original_model
            ),
            media_type=HeaderConstants.TEXT_EVENT_STREAM,
            headers={
                HeaderConstants.CACHE_CONTROL: HeaderConstants.NO_CACHE,
                HeaderConstants.CONNECTION: HeaderConstants.KEEP_ALIVE,
                HeaderConstants.X_ACCEL_BUFFERING: HeaderConstants.NO_BUFFERING
            }
        )
    
    async def _handle_non_stream_response(
        self, 
        k2think_payload: Dict, 
        headers: Dict[str, str],
        output_thinking: bool = True,
        original_model: str = None
    ) -> JSONResponse:
        """å¤„ç†éæµå¼å“åº”"""
        full_content, token_info = await self.response_processor.process_non_stream_response(
            k2think_payload, headers, output_thinking
        )
        
        openai_response = self.response_processor.create_completion_response(
            full_content, token_info, original_model
        )
        
        return JSONResponse(content=openai_response)
    
    async def _handle_stream_response_with_retry(
        self, 
        request: ChatCompletionRequest,
        k2think_payload: Dict,
        output_thinking: bool = True,
        enable_toolify: bool = False,
        max_retries: int = 3
    ) -> StreamingResponse:
        """å¤„ç†æµå¼å“åº”ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        last_exception = None
        
        for attempt in range(max_retries):
            # è·å–ä¸‹ä¸€ä¸ªå¯ç”¨token
            token = self.token_manager.get_next_token()
            if not token:
                # æ ¹æ®æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ›´æ–°æä¾›ä¸åŒçš„é”™è¯¯ä¿¡æ¯
                if Config.ENABLE_TOKEN_AUTO_UPDATE:
                    error_message = "Tokenæ± æš‚æ—¶ä¸ºç©ºï¼Œå¯èƒ½æ­£åœ¨è‡ªåŠ¨æ›´æ–°ä¸­ã€‚è¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥è‡ªåŠ¨æ›´æ–°æœåŠ¡çŠ¶æ€ã€‚"
                    safe_log_warning(logger, "æ²¡æœ‰å¯ç”¨çš„tokenï¼Œå¯èƒ½æ­£åœ¨è‡ªåŠ¨æ›´æ–°ä¸­")
                else:
                    error_message = "æ‰€æœ‰tokenéƒ½å·²å¤±æ•ˆï¼Œè¯·æ£€æŸ¥tokené…ç½®æˆ–é‡æ–°åŠ è½½tokenæ–‡ä»¶ã€‚"
                    safe_log_error(logger, "æ²¡æœ‰å¯ç”¨çš„token")
                
                raise HTTPException(
                    status_code=APIConstants.HTTP_SERVICE_UNAVAILABLE,
                    detail={
                        "error": {
                            "message": error_message,
                            "type": ErrorMessages.API_ERROR
                        }
                    }
                )
            
            # æ„å»ºè¯·æ±‚å¤´
            headers = self._build_request_headers(request, k2think_payload, token)
            
            try:
                safe_log_info(logger, f"å°è¯•æµå¼è¯·æ±‚ (ç¬¬{attempt + 1}æ¬¡)")
                
                # åˆ›å»ºæµå¼ç”Ÿæˆå™¨ï¼Œå†…éƒ¨å¤„ç†tokenæˆåŠŸ/å¤±è´¥æ ‡è®°
                async def stream_generator():
                    try:
                        async for chunk in self.response_processor.process_stream_response(
                            k2think_payload, headers, output_thinking, request.model, enable_toolify
                        ):
                            yield chunk
                        # æµå¼å“åº”æˆåŠŸå®Œæˆï¼Œæ ‡è®°tokenæˆåŠŸ
                        self.token_manager.mark_token_success(token)
                    except Exception as e:
                        # æµå¼å“åº”è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œæ ‡è®°tokenå¤±è´¥
                        safe_log_warning(logger, f"ğŸ” æµå¼å“åº”å¼‚å¸¸è¢«æ•è·ï¼Œå‡†å¤‡æ ‡è®°tokenå¤±è´¥: {str(e)}")
                        
                        # æ ‡è®°tokenå¤±è´¥ï¼ˆè¿™ä¼šè§¦å‘è‡ªåŠ¨åˆ·æ–°é€»è¾‘ï¼‰
                        token_failed = self.token_manager.mark_token_failure(token, str(e))
                        
                        # ç‰¹åˆ«å¤„ç†401é”™è¯¯
                        if "401" in str(e) or "unauthorized" in str(e).lower():
                            safe_log_warning(logger, f"ğŸ”’ æµå¼å“åº”ä¸­æ£€æµ‹åˆ°401è®¤è¯é”™è¯¯ï¼Œtokenæ ‡è®°å¤±è´¥: {token_failed}")
                            safe_log_info(logger, f"ğŸš¨ å·²è°ƒç”¨mark_token_failureï¼Œåº”è¯¥è§¦å‘è‡ªåŠ¨åˆ·æ–°")
                        else:
                            safe_log_warning(logger, f"æµå¼å“åº”ä¸­æ£€æµ‹åˆ°å…¶ä»–é”™è¯¯: {str(e)}")
                        
                        # æ³¨æ„ï¼šä¸é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œé¿å…"response already started"é”™è¯¯
                        # é”™è¯¯ä¿¡æ¯å·²ç»é€šè¿‡response_processorå‘é€ç»™å®¢æˆ·ç«¯
                
                return StreamingResponse(
                    stream_generator(),
                    media_type=HeaderConstants.TEXT_EVENT_STREAM,
                    headers={
                        HeaderConstants.CACHE_CONTROL: HeaderConstants.NO_CACHE,
                        HeaderConstants.CONNECTION: HeaderConstants.KEEP_ALIVE,
                        HeaderConstants.X_ACCEL_BUFFERING: HeaderConstants.NO_BUFFERING
                    }
                )
            except (UpstreamError, Exception) as e:
                # è¿™é‡Œåªå¤„ç†æµå¼å“åº”å¯åŠ¨å‰çš„å¼‚å¸¸ï¼ˆä¸»è¦æ˜¯è¿æ¥é”™è¯¯ï¼‰
                # 401ç­‰ä¸Šæ¸¸æœåŠ¡é”™è¯¯ç°åœ¨åœ¨æµå¼å“åº”å†…éƒ¨å¤„ç†ï¼Œä¸ä¼šåˆ°è¾¾è¿™é‡Œ
                last_exception = e
                safe_log_warning(logger, f"æµå¼è¯·æ±‚å¯åŠ¨å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡): {e}")
                
                # æ ‡è®°tokenå¤±è´¥
                token_failed = self.token_manager.mark_token_failure(token, str(e))
                if token_failed:
                    safe_log_error(logger, f"Tokenå·²è¢«æ ‡è®°ä¸ºå¤±æ•ˆ")
                
                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                if attempt == max_retries - 1:
                    break
                
                # çŸ­æš‚å»¶è¿Ÿåé‡è¯•
                await asyncio.sleep(0.5)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        safe_log_error(logger, "æ‰€æœ‰æµå¼è¯·æ±‚é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæœ€åé”™è¯¯", last_exception)
        raise HTTPException(
            status_code=APIConstants.HTTP_INTERNAL_ERROR,
            detail={
                "error": {
                    "message": f"æµå¼è¯·æ±‚å¤±è´¥: {str(last_exception)}",
                    "type": ErrorMessages.API_ERROR
                }
            }
        )
    
    async def _handle_non_stream_response_with_retry(
        self, 
        request: ChatCompletionRequest,
        k2think_payload: Dict,
        output_thinking: bool = True,
        enable_toolify: bool = False,
        max_retries: int = 3
    ) -> JSONResponse:
        """å¤„ç†éæµå¼å“åº”ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        last_exception = None
        
        for attempt in range(max_retries):
            # è·å–ä¸‹ä¸€ä¸ªå¯ç”¨token
            token = self.token_manager.get_next_token()
            if not token:
                # æ ¹æ®æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ›´æ–°æä¾›ä¸åŒçš„é”™è¯¯ä¿¡æ¯
                if Config.ENABLE_TOKEN_AUTO_UPDATE:
                    error_message = "Tokenæ± æš‚æ—¶ä¸ºç©ºï¼Œå¯èƒ½æ­£åœ¨è‡ªåŠ¨æ›´æ–°ä¸­ã€‚è¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥è‡ªåŠ¨æ›´æ–°æœåŠ¡çŠ¶æ€ã€‚"
                    safe_log_warning(logger, "æ²¡æœ‰å¯ç”¨çš„tokenï¼Œå¯èƒ½æ­£åœ¨è‡ªåŠ¨æ›´æ–°ä¸­")
                else:
                    error_message = "æ‰€æœ‰tokenéƒ½å·²å¤±æ•ˆï¼Œè¯·æ£€æŸ¥tokené…ç½®æˆ–é‡æ–°åŠ è½½tokenæ–‡ä»¶ã€‚"
                    safe_log_error(logger, "æ²¡æœ‰å¯ç”¨çš„token")
                
                raise HTTPException(
                    status_code=APIConstants.HTTP_SERVICE_UNAVAILABLE,
                    detail={
                        "error": {
                            "message": error_message,
                            "type": ErrorMessages.API_ERROR
                        }
                    }
                )
            
            # æ„å»ºè¯·æ±‚å¤´
            headers = self._build_request_headers(request, k2think_payload, token)
            
            try:
                safe_log_info(logger, f"å°è¯•éæµå¼è¯·æ±‚ (ç¬¬{attempt + 1}æ¬¡)")
                
                # å¤„ç†å“åº”
                full_content, token_info = await self.response_processor.process_non_stream_response(
                    k2think_payload, headers, output_thinking
                )
                
                # æ ‡è®°tokenæˆåŠŸ
                self.token_manager.mark_token_success(token)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
                tool_response = None
                if enable_toolify:
                    from src.toolify_handler import parse_toolify_response
                    tool_response = parse_toolify_response(full_content, request.model)
                
                if tool_response:
                    # è¿”å›åŒ…å«tool_callsçš„å“åº”
                    openai_response = {
                        "id": f"chatcmpl-{int(time.time())}",
                        "object": ResponseConstants.CHAT_COMPLETION_OBJECT,
                        "created": int(time.time()),
                        "model": request.model,
                        "choices": [{
                            "index": 0,
                            "message": tool_response,
                            "finish_reason": "tool_calls"
                        }],
                        "usage": token_info or {
                            "prompt_tokens": 0,
                            "completion_tokens": 0,
                            "total_tokens": 0
                        }
                    }
                else:
                    openai_response = self.response_processor.create_completion_response(
                        full_content, token_info, request.model
                    )
                
                return JSONResponse(content=openai_response)
                
            except (UpstreamError, Exception) as e:
                last_exception = e
                
                # ç‰¹åˆ«å¤„ç†401é”™è¯¯
                if "401" in str(e) or "unauthorized" in str(e).lower():
                    safe_log_warning(logger, f"ğŸ”’ éæµå¼è¯·æ±‚é‡åˆ°401è®¤è¯é”™è¯¯ (ç¬¬{attempt + 1}æ¬¡): {e}")
                    
                    # å¯¹äº401é”™è¯¯ï¼Œå¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å°è¯•ï¼Œè¿”å›å‹å¥½æ¶ˆæ¯è€Œä¸é‡è¯•
                    if attempt == 0:
                        # æ ‡è®°tokenå¤±è´¥ä»¥è§¦å‘è‡ªåŠ¨åˆ·æ–°
                        self.token_manager.mark_token_failure(token, str(e))
                        
                        # è¿”å›å‹å¥½çš„åˆ·æ–°æç¤ºæ¶ˆæ¯
                        openai_response = self.response_processor.create_completion_response(
                            content="ğŸ”„ tokenså¼ºåˆ¶åˆ·æ–°å·²å¯åŠ¨ï¼Œè¯·ç¨åå†è¯•",
                            token_info={
                                "prompt_tokens": 0,
                                "completion_tokens": 10,
                                "total_tokens": 10
                            },
                            model=request.model
                        )
                        return JSONResponse(content=openai_response)
                else:
                    safe_log_warning(logger, f"éæµå¼è¯·æ±‚å¤±è´¥ (ç¬¬{attempt + 1}æ¬¡): {e}")
                
                # æ ‡è®°tokenå¤±è´¥
                token_failed = self.token_manager.mark_token_failure(token, str(e))
                if token_failed:
                    safe_log_error(logger, f"Tokenå·²è¢«æ ‡è®°ä¸ºå¤±æ•ˆ")
                
                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                if attempt == max_retries - 1:
                    break
                
                # çŸ­æš‚å»¶è¿Ÿåé‡è¯•
                await asyncio.sleep(0.5)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†
        safe_log_error(logger, "æ‰€æœ‰éæµå¼è¯·æ±‚é‡è¯•éƒ½å¤±è´¥äº†ï¼Œæœ€åé”™è¯¯", last_exception)
        raise HTTPException(
            status_code=APIConstants.HTTP_INTERNAL_ERROR,
            detail={
                "error": {
                    "message": f"éæµå¼è¯·æ±‚å¤±è´¥: {str(last_exception)}",
                    "type": ErrorMessages.API_ERROR
                }
            }
        )