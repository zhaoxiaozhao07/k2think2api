"""
å“åº”å¤„ç†æ¨¡å—
å¤„ç†æµå¼å’Œéæµå¼å“åº”çš„æ‰€æœ‰é€»è¾‘
"""
import json
import time
import asyncio
import logging
import uuid
from datetime import datetime
from typing import Dict, AsyncGenerator, Tuple, Optional
import pytz
import httpx

from src.constants import (
    APIConstants, ResponseConstants, ContentConstants, 
    NumericConstants, TimeConstants, HeaderConstants
)
from src.exceptions import UpstreamError, TimeoutError as ProxyTimeoutError
from src.utils import safe_log_error, safe_log_info, safe_log_warning
from src.toolify_config import get_toolify
from src.toolify.detector import StreamingFunctionCallDetector

logger = logging.getLogger(__name__)

class ResponseProcessor:
    """å“åº”å¤„ç†å™¨"""
    
    def __init__(self, config):
        self.config = config
    
    def extract_answer_content(self, full_content: str, output_thinking: bool = True) -> str:
        """åˆ é™¤ç¬¬ä¸€ä¸ª<answer>æ ‡ç­¾å’Œæœ€åä¸€ä¸ª</answer>æ ‡ç­¾ï¼Œä¿ç•™å†…å®¹"""
        if not full_content:
            return full_content
        
        # å®Œå…¨é€šè¿‡æ¨¡å‹åæ§åˆ¶æ€è€ƒå†…å®¹è¾“å‡ºï¼Œé»˜è®¤æ˜¾ç¤ºæ€è€ƒå†…å®¹
        should_output_thinking = output_thinking
        
        if should_output_thinking:
            # åˆ é™¤ç¬¬ä¸€ä¸ª<answer>
            answer_start = full_content.find(ContentConstants.ANSWER_START_TAG)
            if answer_start != -1:
                full_content = full_content[:answer_start] + full_content[answer_start + len(ContentConstants.ANSWER_START_TAG):]

            # åˆ é™¤æœ€åä¸€ä¸ª</answer>
            answer_end = full_content.rfind(ContentConstants.ANSWER_END_TAG)
            if answer_end != -1:
                full_content = full_content[:answer_end] + full_content[answer_end + len(ContentConstants.ANSWER_END_TAG):]

            return full_content.strip()
        else:
            # åˆ é™¤<think>éƒ¨åˆ†ï¼ˆåŒ…æ‹¬æ ‡ç­¾ï¼‰
            think_start = full_content.find(ContentConstants.THINK_START_TAG)
            think_end = full_content.find(ContentConstants.THINK_END_TAG)
            if think_start != -1 and think_end != -1:
                full_content = full_content[:think_start] + full_content[think_end + len(ContentConstants.THINK_END_TAG):]
            
            # åˆ é™¤<answer>æ ‡ç­¾åŠå…¶å†…å®¹ä¹‹å¤–çš„éƒ¨åˆ†
            answer_start = full_content.find(ContentConstants.ANSWER_START_TAG)
            answer_end = full_content.rfind(ContentConstants.ANSWER_END_TAG)
            if answer_start != -1 and answer_end != -1:
                content = full_content[answer_start + len(ContentConstants.ANSWER_START_TAG):answer_end]
                return content.strip()

            return full_content.strip()
    
    def calculate_dynamic_chunk_size(self, content_length: int) -> int:
        """
        åŠ¨æ€è®¡ç®—æµå¼è¾“å‡ºçš„chunkå¤§å°
        ç¡®ä¿æ€»è¾“å‡ºæ—¶é—´ä¸è¶…è¿‡MAX_STREAM_TIMEç§’
        
        Args:
            content_length: å¾…è¾“å‡ºå†…å®¹çš„æ€»é•¿åº¦
        
        Returns:
            int: åŠ¨æ€è®¡ç®—çš„chunkå¤§å°ï¼Œæœ€å°ä¸º50
        """
        if content_length <= 0:
            return self.config.STREAM_CHUNK_SIZE
        
        # è®¡ç®—éœ€è¦çš„æ€»chunkæ•°é‡ä»¥æ»¡è¶³æ—¶é—´é™åˆ¶
        # æ€»æ—¶é—´ = chunkæ•°é‡ * STREAM_DELAY
        # chunkæ•°é‡ = content_length / chunk_size
        # æ‰€ä»¥ï¼šæ€»æ—¶é—´ = (content_length / chunk_size) * STREAM_DELAY
        # è§£å‡ºï¼šchunk_size = (content_length * STREAM_DELAY) / MAX_STREAM_TIME
        
        calculated_chunk_size = int((content_length * self.config.STREAM_DELAY) / self.config.MAX_STREAM_TIME)
        
        # ç¡®ä¿chunk_sizeä¸å°äºæœ€å°å€¼
        dynamic_chunk_size = max(calculated_chunk_size, NumericConstants.MIN_CHUNK_SIZE)
        
        # å¦‚æœè®¡ç®—å‡ºçš„chunk_sizeå¤ªå¤§ï¼ˆæ¯”å¦‚å†…å®¹å¾ˆçŸ­ï¼‰ï¼Œä½¿ç”¨é»˜è®¤å€¼
        if dynamic_chunk_size > content_length:
            dynamic_chunk_size = min(self.config.STREAM_CHUNK_SIZE, content_length)
        
        logger.debug(f"åŠ¨æ€chunk_sizeè®¡ç®—: å†…å®¹é•¿åº¦={content_length}, è®¡ç®—å€¼={calculated_chunk_size}, æœ€ç»ˆå€¼={dynamic_chunk_size}")
        
        return dynamic_chunk_size
    
    def content_to_multimodal(self, content) -> str | list[dict]:
        """å°†å†…å®¹è½¬æ¢ä¸ºå¤šæ¨¡æ€æ ¼å¼ç”¨äºK2Think API"""
        if content is None:
            return ""
        if isinstance(content, str):
            return content
        if isinstance(content, list):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒå†…å®¹
            has_image = False
            result_parts = []
            
            for p in content:
                if hasattr(p, 'type'):  # ContentPart object
                    if getattr(p, 'type') == ContentConstants.TEXT_TYPE and getattr(p, 'text', None):
                        result_parts.append({
                            "type": ContentConstants.TEXT_TYPE,
                            "text": getattr(p, 'text')
                        })
                    elif getattr(p, 'type') == ContentConstants.IMAGE_URL_TYPE and getattr(p, 'image_url', None):
                        has_image = True
                        image_url_obj = getattr(p, 'image_url')
                        if hasattr(image_url_obj, 'url'):
                            url = getattr(image_url_obj, 'url')
                        else:
                            url = image_url_obj.get('url') if isinstance(image_url_obj, dict) else str(image_url_obj)
                        
                        result_parts.append({
                            "type": ContentConstants.IMAGE_URL_TYPE,
                            "image_url": {
                                "url": url
                            }
                        })
                elif isinstance(p, dict):
                    if p.get("type") == ContentConstants.TEXT_TYPE and p.get("text"):
                        result_parts.append({
                            "type": ContentConstants.TEXT_TYPE, 
                            "text": p.get("text")
                        })
                    elif p.get("type") == ContentConstants.IMAGE_URL_TYPE and p.get("image_url"):
                        has_image = True
                        result_parts.append({
                            "type": ContentConstants.IMAGE_URL_TYPE,
                            "image_url": p.get("image_url")
                        })
                elif isinstance(p, str):
                    result_parts.append({
                        "type": ContentConstants.TEXT_TYPE,
                        "text": p
                    })
            
            # å¦‚æœåŒ…å«å›¾åƒï¼Œè¿”å›å¤šæ¨¡æ€æ ¼å¼ï¼›å¦åˆ™è¿”å›çº¯æ–‡æœ¬
            if has_image and result_parts:
                return result_parts
            else:
                # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
                text_parts = []
                for part in result_parts:
                    if part.get("type") == ContentConstants.TEXT_TYPE:
                        text_parts.append(part.get("text", ""))
                return " ".join(text_parts)
        
        # å¤„ç†å…¶ä»–ç±»å‹
        try:
            return str(content)
        except:
            return ""
    
    def get_current_datetime_info(self) -> Dict[str, str]:
        """è·å–å½“å‰æ—¶é—´ä¿¡æ¯"""
        # è®¾ç½®æ—¶åŒºä¸ºä¸Šæµ·
        tz = pytz.timezone(ContentConstants.DEFAULT_TIMEZONE)
        now = datetime.now(tz)
        
        return {
            "{{USER_NAME}}": ContentConstants.DEFAULT_USER_NAME,
            "{{USER_LOCATION}}": ContentConstants.DEFAULT_USER_LOCATION,
            "{{CURRENT_DATETIME}}": now.strftime(TimeConstants.DATETIME_FORMAT),
            "{{CURRENT_DATE}}": now.strftime(TimeConstants.DATE_FORMAT),
            "{{CURRENT_TIME}}": now.strftime(TimeConstants.TIME_FORMAT),
            "{{CURRENT_WEEKDAY}}": now.strftime(TimeConstants.WEEKDAY_FORMAT),
            "{{CURRENT_TIMEZONE}}": ContentConstants.DEFAULT_TIMEZONE,
            "{{USER_LANGUAGE}}": ContentConstants.DEFAULT_USER_LANGUAGE
        }
    
    def generate_session_id(self) -> str:
        """ç”Ÿæˆä¼šè¯ID"""
        return str(uuid.uuid4())
    
    def generate_chat_id(self) -> str:
        """ç”ŸæˆèŠå¤©ID"""
        return str(uuid.uuid4())
    
    async def create_http_client(self) -> httpx.AsyncClient:
        """åˆ›å»ºHTTPå®¢æˆ·ç«¯"""
        base_kwargs = {
            "timeout": httpx.Timeout(timeout=None, connect=10.0),
            "limits": httpx.Limits(
                max_keepalive_connections=self.config.MAX_KEEPALIVE_CONNECTIONS, 
                max_connections=self.config.MAX_CONNECTIONS
            ),
            "follow_redirects": True
        }
        
        try:
            return httpx.AsyncClient(**base_kwargs)
        except Exception as e:
            safe_log_error(logger, "åˆ›å»ºå®¢æˆ·ç«¯å¤±è´¥", e)
            raise e
    
    async def make_request(
        self, 
        method: str, 
        url: str, 
        headers: dict, 
        json_data: dict = None, 
        stream: bool = False
    ) -> httpx.Response:
        """å‘é€HTTPè¯·æ±‚"""
        client = None
        
        try:
            client = await self.create_http_client()
            
            if stream:
                # æµå¼è¯·æ±‚è¿”å›context manager
                return client.stream(method, url, headers=headers, json=json_data, timeout=None)
            else:
                response = await client.request(
                    method, url, headers=headers, json=json_data, 
                    timeout=self.config.REQUEST_TIMEOUT
                )
                
                # è¯¦ç»†è®°å½•é200å“åº”
                if response.status_code != APIConstants.HTTP_OK:
                    safe_log_error(logger, f"ä¸Šæ¸¸APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}")
                    safe_log_error(logger, f"å“åº”å¤´: {dict(response.headers)}")
                    try:
                        error_body = response.text
                        safe_log_error(logger, f"é”™è¯¯å“åº”ä½“: {error_body}")
                    except:
                        safe_log_error(logger, "æ— æ³•è¯»å–é”™è¯¯å“åº”ä½“")
                
                response.raise_for_status()
                return response
                
        except httpx.HTTPStatusError as e:
            safe_log_error(logger, f"HTTPçŠ¶æ€é”™è¯¯: {e.response.status_code} - {e.response.text}")
            if client and not stream:
                await client.aclose()
            raise UpstreamError(f"ä¸Šæ¸¸æœåŠ¡é”™è¯¯: {e.response.status_code}", e.response.status_code)
        except httpx.TimeoutException as e:
            safe_log_error(logger, "è¯·æ±‚è¶…æ—¶", e)
            if client and not stream:
                await client.aclose()
            raise ProxyTimeoutError("è¯·æ±‚è¶…æ—¶")
        except Exception as e:
            safe_log_error(logger, "è¯·æ±‚å¼‚å¸¸", e)
            if client and not stream:
                await client.aclose()
            raise e
    
    async def process_non_stream_response(self, k2think_payload: dict, headers: dict, output_thinking: bool = None) -> Tuple[str, dict]:
        """å¤„ç†éæµå¼å“åº”"""
        try:
            response = await self.make_request(
                "POST", 
                self.config.K2THINK_API_URL, 
                headers, 
                k2think_payload, 
                stream=False
            )
            
            # K2Think éæµå¼è¯·æ±‚è¿”å›æ ‡å‡†JSONæ ¼å¼
            result = response.json()
            
            # æå–å†…å®¹
            full_content = ""
            if result.get('choices') and len(result['choices']) > 0:
                choice = result['choices'][0]
                if choice.get('message') and choice['message'].get('content'):
                    raw_content = choice['message']['content']
                    # æå–<answer>æ ‡ç­¾ä¸­çš„å†…å®¹ï¼Œå»é™¤æ ‡ç­¾
                    full_content = self.extract_answer_content(raw_content, output_thinking)
            
            # æå–tokenä¿¡æ¯
            token_info = result.get('usage', {
                "prompt_tokens": NumericConstants.DEFAULT_PROMPT_TOKENS, 
                "completion_tokens": NumericConstants.DEFAULT_COMPLETION_TOKENS, 
                "total_tokens": NumericConstants.DEFAULT_TOTAL_TOKENS
            })
            
            await response.aclose()
            return full_content, token_info
                        
        except Exception as e:
            safe_log_error(logger, "å¤„ç†éæµå¼å“åº”é”™è¯¯", e)
            raise
    
    async def process_stream_response(
        self, 
        k2think_payload: dict, 
        headers: dict,
        output_thinking: bool = None,
        original_model: str = None,
        enable_toolify: bool = False
    ) -> AsyncGenerator[str, None]:
        """å¤„ç†æµå¼å“åº”"""
        try:
            # å‘é€å¼€å§‹chunk
            start_chunk = self._create_chunk_data(
                delta={"role": "assistant", "content": ""},
                finish_reason=None,
                model=original_model
            )
            yield f"{ResponseConstants.STREAM_DATA_PREFIX}{json.dumps(start_chunk)}\n\n"
            
            # ä¼˜åŒ–çš„æ¨¡æ‹Ÿæµå¼è¾“å‡º - ç«‹å³å¼€å§‹è·å–å“åº”å¹¶æµå¼å‘é€
            k2think_payload_copy = k2think_payload.copy()
            k2think_payload_copy["stream"] = False
            
            headers_copy = headers.copy()
            headers_copy[HeaderConstants.ACCEPT] = HeaderConstants.APPLICATION_JSON
            
            # è·å–å®Œæ•´å“åº”
            full_content, token_info = await self.process_non_stream_response(k2think_payload_copy, headers_copy, output_thinking)
            
            if not full_content:
                yield ResponseConstants.STREAM_DONE_MARKER
                return
            
            # æ£€æµ‹å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            toolify_detector = None
            if enable_toolify:
                toolify = get_toolify()
                if toolify:
                    toolify_detector = StreamingFunctionCallDetector(toolify.trigger_signal)
                    safe_log_info(logger, "[TOOLIFY] æµå¼å·¥å…·è°ƒç”¨æ£€æµ‹å™¨å·²åˆå§‹åŒ–")
            
            # å‘é€å†…å®¹ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨æ£€æµ‹ï¼‰
            if toolify_detector:
                # ä½¿ç”¨å·¥å…·è°ƒç”¨æ£€æµ‹å™¨å¤„ç†å†…å®¹
                async for chunk in self._stream_content_with_tool_detection(
                    full_content, original_model, toolify_detector, k2think_payload.get("chat_id", "")
                ):
                    yield chunk
            else:
                # æ­£å¸¸æµå¼å‘é€
                async for chunk in self._stream_content(full_content, original_model):
                    yield chunk
                
                # å‘é€ç»“æŸchunk
                end_chunk = self._create_chunk_data(
                    delta={},
                    finish_reason=ResponseConstants.FINISH_REASON_STOP,
                    model=original_model
                )
                yield f"{ResponseConstants.STREAM_DATA_PREFIX}{json.dumps(end_chunk)}\n\n"
                yield ResponseConstants.STREAM_DONE_MARKER
            
        except Exception as e:
            safe_log_error(logger, "æµå¼å“åº”å¤„ç†é”™è¯¯", e)
            
            # å‘é€é”™è¯¯ä¿¡æ¯ä½œä¸ºæµå¼å“åº”çš„ä¸€éƒ¨åˆ†ï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            if "401" in str(e) or "unauthorized" in str(e).lower():
                # 401é”™è¯¯ï¼šæ˜¾ç¤ºtokenså¼ºåˆ¶åˆ·æ–°æ¶ˆæ¯
                error_message = "ğŸ”„ tokenså¼ºåˆ¶åˆ·æ–°å·²å¯åŠ¨ï¼Œè¯·ç¨åå†è¯•"
                safe_log_info(logger, "æ£€æµ‹åˆ°401é”™è¯¯ï¼Œå‘å®¢æˆ·ç«¯å‘é€å¼ºåˆ¶åˆ·æ–°æç¤º")
            else:
                # å…¶ä»–é”™è¯¯ï¼šæ˜¾ç¤ºä¸€èˆ¬é”™è¯¯ä¿¡æ¯
                error_message = f"è¯·æ±‚å¤„ç†å¤±è´¥: {str(e)}"
            
            # å‘é€é”™è¯¯å†…å®¹ä½œä¸ºæ­£å¸¸çš„æµå¼å“åº”
            error_chunk = self._create_chunk_data(
                delta={"content": f"\n\n{error_message}"},
                finish_reason=None,
                model=original_model
            )
            yield f"{ResponseConstants.STREAM_DATA_PREFIX}{json.dumps(error_chunk)}\n\n"
            
            # å‘é€ç»“æŸchunk
            end_chunk = self._create_chunk_data(
                delta={},
                finish_reason=ResponseConstants.FINISH_REASON_ERROR,
                model=original_model
            )
            yield f"{ResponseConstants.STREAM_DATA_PREFIX}{json.dumps(end_chunk)}\n\n"
            yield ResponseConstants.STREAM_DONE_MARKER
            
            # é‡æ–°æŠ›å‡ºå¼‚å¸¸ä»¥ä¾¿ä¸Šå±‚å¤„ç†tokenå¤±è´¥ï¼ˆåœ¨å‘é€å‹å¥½æ¶ˆæ¯ä¹‹åï¼‰
            # ä¸Šå±‚ä¼šæ•è·è¿™ä¸ªå¼‚å¸¸å¹¶è°ƒç”¨token_manager.mark_token_failure
            raise e
    
    async def _stream_content(self, content: str, model: str = None) -> AsyncGenerator[str, None]:
        """æµå¼å‘é€å†…å®¹"""
        chunk_size = self.calculate_dynamic_chunk_size(len(content))
        
        for i in range(0, len(content), chunk_size):
            chunk_content = content[i:i + chunk_size]
            
            chunk = self._create_chunk_data(
                delta={"content": chunk_content},
                finish_reason=None,
                model=model
            )
            
            yield f"{ResponseConstants.STREAM_DATA_PREFIX}{json.dumps(chunk)}\n\n"
            # æ·»åŠ å»¶è¿Ÿæ¨¡æ‹ŸçœŸå®æµå¼æ•ˆæœ
            await asyncio.sleep(self.config.STREAM_DELAY)
    
    async def _stream_content_with_tool_detection(
        self, 
        content: str, 
        model: str, 
        detector: StreamingFunctionCallDetector,
        chat_id: str
    ) -> AsyncGenerator[str, None]:
        """æµå¼å‘é€å†…å®¹å¹¶æ£€æµ‹å·¥å…·è°ƒç”¨"""
        chunk_size = self.calculate_dynamic_chunk_size(len(content))
        
        for i in range(0, len(content), chunk_size):
            chunk_content = content[i:i + chunk_size]
            
            # ä½¿ç”¨æ£€æµ‹å™¨å¤„ç†chunk
            is_tool_detected, content_to_yield = detector.process_chunk(chunk_content)
            
            if is_tool_detected:
                safe_log_info(logger, "[TOOLIFY] æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨è§¦å‘ä¿¡å·")
            
            # è¾“å‡ºå¤„ç†åçš„å†…å®¹
            if content_to_yield:
                chunk = self._create_chunk_data(
                    delta={"content": content_to_yield},
                    finish_reason=None,
                    model=model
                )
                yield f"{ResponseConstants.STREAM_DATA_PREFIX}{json.dumps(chunk)}\n\n"
            
            await asyncio.sleep(self.config.STREAM_DELAY)
        
        # æµç»“æŸæ—¶çš„æœ€ç»ˆå¤„ç†
        parsed_tools, remaining_content = detector.finalize()
        
        # è¾“å‡ºå‰©ä½™å†…å®¹
        if remaining_content:
            safe_log_info(logger, f"[TOOLIFY] è¾“å‡ºç¼“å†²åŒºå‰©ä½™å†…å®¹: {len(remaining_content)}å­—ç¬¦")
            chunk = self._create_chunk_data(
                delta={"content": remaining_content},
                finish_reason=None,
                model=model
            )
            yield f"{ResponseConstants.STREAM_DATA_PREFIX}{json.dumps(chunk)}\n\n"
        
        # å¦‚æœæ£€æµ‹åˆ°å·¥å…·è°ƒç”¨ï¼Œè¾“å‡ºå·¥å…·è°ƒç”¨ç»“æœ
        if parsed_tools:
            safe_log_info(logger, f"[TOOLIFY] æ£€æµ‹åˆ° {len(parsed_tools)} ä¸ªå·¥å…·è°ƒç”¨")
            from src.toolify_handler import format_toolify_response_for_stream
            tool_chunks = format_toolify_response_for_stream(parsed_tools, model, chat_id)
            for chunk in tool_chunks:
                yield chunk
        else:
            # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œæ­£å¸¸ç»“æŸ
            end_chunk = self._create_chunk_data(
                delta={},
                finish_reason=ResponseConstants.FINISH_REASON_STOP,
                model=model
            )
            yield f"{ResponseConstants.STREAM_DATA_PREFIX}{json.dumps(end_chunk)}\n\n"
            yield ResponseConstants.STREAM_DONE_MARKER
    
    def _create_chunk_data(self, delta: dict, finish_reason: Optional[str], model: str = None) -> dict:
        """åˆ›å»ºæµå¼å“åº”chunkæ•°æ®"""
        return {
            "id": f"chatcmpl-{int(time.time() * 1000)}",
            "object": ResponseConstants.CHAT_COMPLETION_CHUNK_OBJECT,
            "created": int(time.time()),
            "model": model or APIConstants.MODEL_ID,
            "choices": [{
                "index": 0,
                "delta": delta,
                "finish_reason": finish_reason
            }]
        }
    
    def create_completion_response(
        self, 
        content: Optional[str],
        token_info: Optional[dict] = None,
        model: str = None
    ) -> dict:
        """åˆ›å»ºå®Œæ•´çš„èŠå¤©è¡¥å…¨å“åº”"""
        message = {
            "role": "assistant",
            "content": content,
        }
        
        return {
            "id": f"chatcmpl-{int(time.time())}",
            "object": ResponseConstants.CHAT_COMPLETION_OBJECT,
            "created": int(time.time()),
            "model": model or APIConstants.MODEL_ID,
            "choices": [{
                "index": 0,
                "message": message,
                "finish_reason": ResponseConstants.FINISH_REASON_STOP
            }],
            "usage": token_info or {
                "prompt_tokens": NumericConstants.DEFAULT_PROMPT_TOKENS,
                "completion_tokens": NumericConstants.DEFAULT_COMPLETION_TOKENS,
                "total_tokens": NumericConstants.DEFAULT_TOTAL_TOKENS
            }
        }